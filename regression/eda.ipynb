{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-05-06T17:10:13.300893Z",
     "end_time": "2023-05-06T17:10:35.680200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_194308/4215461173.py:5: DtypeWarning: Columns (0,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  social = pd.read_csv(\"Aggregated_reddit_twitter.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "social = pd.read_csv(\"Aggregated_reddit_twitter.csv\")\n",
    "social = social.rename(columns={\"date\": \"timestamp\"})\n",
    "social[\"timestamp\"] = pd.to_datetime(social[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "social = social[social.content != \"u/circular360\"]\n",
    "social = social[social.content != \"There you go\"]\n",
    "social = social[\n",
    "    social.content != \"Why force the other player to stay around? Just give the winning player the option to continue the game with a perma passing AI controlling the surrendering player.\"]\n",
    "# crypto = pd.read_csv(\"transformed_crypto.csv\")\n",
    "# crypto[\"timestamp\"] = pd.to_datetime(crypto[\"timestamp\"]).dt.tz_localize(None)\n",
    "crypto = pd.read_csv(\"crypto_differences.csv\")\n",
    "crypto[\"timestamp\"] = pd.to_datetime(crypto[\"timestamp\"]).dt.tz_localize(None)\n",
    "\n",
    "social = social.sort_values(by=\"timestamp\")\n",
    "# social.dtypes\n",
    "sr = social.resample(\"T\", on=\"timestamp\").size()\n",
    "freq = sr.reset_index()\n",
    "\n",
    "freq = freq.rename(columns={0: \"n_comments\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T17:10:35.951790Z",
     "end_time": "2023-05-06T17:10:51.444595Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                  timestamp  n_comments  hour  day  month\n0       2021-03-01 00:00:00          18     0    0      3\n1       2021-03-01 00:01:00           8     0    0      3\n2       2021-03-01 00:02:00           4     0    0      3\n3       2021-03-01 00:03:00          11     0    0      3\n4       2021-03-01 00:04:00           4     0    0      3\n...                     ...         ...   ...  ...    ...\n1132079 2023-04-26 03:59:00           0     3    2      4\n1132080 2023-04-26 04:00:00           0     4    2      4\n1132081 2023-04-26 04:01:00           0     4    2      4\n1132082 2023-04-26 04:02:00           0     4    2      4\n1132083 2023-04-26 04:03:00           1     4    2      4\n\n[1132084 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>timestamp</th>\n      <th>n_comments</th>\n      <th>hour</th>\n      <th>day</th>\n      <th>month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-03-01 00:00:00</td>\n      <td>18</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021-03-01 00:01:00</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2021-03-01 00:02:00</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2021-03-01 00:03:00</td>\n      <td>11</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2021-03-01 00:04:00</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1132079</th>\n      <td>2023-04-26 03:59:00</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1132080</th>\n      <td>2023-04-26 04:00:00</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1132081</th>\n      <td>2023-04-26 04:01:00</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1132082</th>\n      <td>2023-04-26 04:02:00</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1132083</th>\n      <td>2023-04-26 04:03:00</td>\n      <td>1</td>\n      <td>4</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>1132084 rows Ã— 5 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq[\"hour\"] = freq[\"timestamp\"].dt.hour\n",
    "freq[\"day\"] = freq[\"timestamp\"].dt.dayofweek\n",
    "freq[\"month\"] = freq[\"timestamp\"].dt.month\n",
    "freq"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T17:10:51.444460Z",
     "end_time": "2023-05-06T17:10:51.493470Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25466, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                  timestamp  n_comments  hour  day  month   \n526378  2022-03-01 12:58:00           0    12    1      3  \\\n527070  2022-03-02 00:30:00           0     0    2      3   \n527192  2022-03-02 02:32:00           0     2    2      3   \n527520  2022-03-02 08:00:00           0     8    2      3   \n527640  2022-03-02 10:00:00           0    10    2      3   \n...                     ...         ...   ...  ...    ...   \n1012170 2023-02-01 21:30:00           0    21    2      2   \n1012203 2023-02-01 22:03:00           0    22    2      2   \n1012230 2023-02-01 22:30:00           0    22    2      2   \n1012259 2023-02-01 22:59:00           0    22    2      2   \n1012285 2023-02-01 23:25:00           0    23    2      2   \n\n         overall_sentiment_score  BTC_sentiment  ETH_sentiment  BTC_relevance   \n526378                 -0.138797      -0.118125       0.000000       0.018982  \\\n527070                  0.021230       0.000000       0.000892       0.000000   \n527192                 -0.031243       0.006883       0.000000       0.063967   \n527520                 -0.174572      -0.009523       0.000000       0.125701   \n527640                 -0.036470       0.001776       0.001233       0.162677   \n...                          ...            ...            ...            ...   \n1012170                 0.089214       0.283323       0.000000       0.630142   \n1012203                 0.038911      -0.091982       0.000000       0.046139   \n1012230                 0.162278       0.058989       0.000000       0.056275   \n1012259                -0.033137       0.000000      -0.083125       0.000000   \n1012285                 0.255075       0.000000       0.413421       0.000000   \n\n         ETH_relevance  \n526378        0.000000  \n527070        0.053967  \n527192        0.000000  \n527520        0.000000  \n527640        0.023400  \n...                ...  \n1012170       0.000000  \n1012203       0.000000  \n1012230       0.000000  \n1012259       0.185306  \n1012285       0.951742  \n\n[25466 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>timestamp</th>\n      <th>n_comments</th>\n      <th>hour</th>\n      <th>day</th>\n      <th>month</th>\n      <th>overall_sentiment_score</th>\n      <th>BTC_sentiment</th>\n      <th>ETH_sentiment</th>\n      <th>BTC_relevance</th>\n      <th>ETH_relevance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>526378</th>\n      <td>2022-03-01 12:58:00</td>\n      <td>0</td>\n      <td>12</td>\n      <td>1</td>\n      <td>3</td>\n      <td>-0.138797</td>\n      <td>-0.118125</td>\n      <td>0.000000</td>\n      <td>0.018982</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>527070</th>\n      <td>2022-03-02 00:30:00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0.021230</td>\n      <td>0.000000</td>\n      <td>0.000892</td>\n      <td>0.000000</td>\n      <td>0.053967</td>\n    </tr>\n    <tr>\n      <th>527192</th>\n      <td>2022-03-02 02:32:00</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>-0.031243</td>\n      <td>0.006883</td>\n      <td>0.000000</td>\n      <td>0.063967</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>527520</th>\n      <td>2022-03-02 08:00:00</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2</td>\n      <td>3</td>\n      <td>-0.174572</td>\n      <td>-0.009523</td>\n      <td>0.000000</td>\n      <td>0.125701</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>527640</th>\n      <td>2022-03-02 10:00:00</td>\n      <td>0</td>\n      <td>10</td>\n      <td>2</td>\n      <td>3</td>\n      <td>-0.036470</td>\n      <td>0.001776</td>\n      <td>0.001233</td>\n      <td>0.162677</td>\n      <td>0.023400</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1012170</th>\n      <td>2023-02-01 21:30:00</td>\n      <td>0</td>\n      <td>21</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.089214</td>\n      <td>0.283323</td>\n      <td>0.000000</td>\n      <td>0.630142</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1012203</th>\n      <td>2023-02-01 22:03:00</td>\n      <td>0</td>\n      <td>22</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.038911</td>\n      <td>-0.091982</td>\n      <td>0.000000</td>\n      <td>0.046139</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1012230</th>\n      <td>2023-02-01 22:30:00</td>\n      <td>0</td>\n      <td>22</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.162278</td>\n      <td>0.058989</td>\n      <td>0.000000</td>\n      <td>0.056275</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1012259</th>\n      <td>2023-02-01 22:59:00</td>\n      <td>0</td>\n      <td>22</td>\n      <td>2</td>\n      <td>2</td>\n      <td>-0.033137</td>\n      <td>0.000000</td>\n      <td>-0.083125</td>\n      <td>0.000000</td>\n      <td>0.185306</td>\n    </tr>\n    <tr>\n      <th>1012285</th>\n      <td>2023-02-01 23:25:00</td>\n      <td>0</td>\n      <td>23</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.255075</td>\n      <td>0.000000</td>\n      <td>0.413421</td>\n      <td>0.000000</td>\n      <td>0.951742</td>\n    </tr>\n  </tbody>\n</table>\n<p>25466 rows Ã— 10 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alp = pd.read_csv(\"df_alphavantage_scores.csv\")\n",
    "alp[\"timestamp\"] = pd.to_datetime(alp[\"timestamp\"])\n",
    "alp = alp.drop(\"Unnamed: 0\", axis=1)\n",
    "print(alp.shape)\n",
    "\n",
    "# alp = alp.fillna(0)\n",
    "frqn = pd.merge(freq, alp, on=\"timestamp\", how=\"left\")\n",
    "frqn = frqn.dropna(how=\"all\", subset=[\"overall_sentiment_score\", \"BTC_relevance\", \"BTC_sentiment\", \"ETH_relevance\",\n",
    "                                      \"ETH_sentiment\"])\n",
    "frqn = frqn.fillna(0)\n",
    "frqn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T17:10:51.493642Z",
     "end_time": "2023-05-06T17:10:51.852700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "mg = pd.merge(crypto, frqn, on=\"timestamp\", how=\"inner\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T17:10:51.854135Z",
     "end_time": "2023-05-06T17:10:52.041246Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(15183, 16)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "mg.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T17:10:52.042391Z",
     "end_time": "2023-05-06T17:10:53.199934Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "       Unnamed: 0  XRP-PERP  SOL-PERP  ETH-PERP  DOGE-PERP  BTC-PERP   \n0          611127  0.030873  0.003136 -0.003228   0.000491  0.008013  \\\n1          611819  0.000000 -0.001285  0.006197  -0.000774  0.028552   \n2          611941  0.004518  0.008197  0.001328   0.000801  0.006267   \n3          612269  0.012173  0.002481  0.000525  -0.006094  0.014352   \n4          612389 -0.002718 -0.002229  0.000740   0.001686 -0.002714   \n...           ...       ...       ...       ...        ...       ...   \n15178      875067  0.000451  0.000326  0.000977  -0.000086  0.000418   \n15179      875071 -0.001382  0.000054 -0.000488  -0.000031  0.000070   \n15180      875094 -0.000662 -0.000571 -0.000490  -0.000508 -0.000465   \n15181      875118  0.000090  0.000981  0.000692  -0.000022  0.000978   \n15182      875221  0.000718  0.000780 -0.000580   0.000807  0.000185   \n\n                timestamp  n_comments  hour  day  month   \n0     2022-03-01 12:58:00           0    12    1      3  \\\n1     2022-03-02 00:30:00           0     0    2      3   \n2     2022-03-02 02:32:00           0     2    2      3   \n3     2022-03-02 08:00:00           0     8    2      3   \n4     2022-03-02 10:00:00           0    10    2      3   \n...                   ...         ...   ...  ...    ...   \n15178 2022-08-31 20:41:00           0    20    2      8   \n15179 2022-08-31 20:45:00           0    20    2      8   \n15180 2022-08-31 21:08:00           0    21    2      8   \n15181 2022-08-31 21:32:00           0    21    2      8   \n15182 2022-08-31 23:15:00           0    23    2      8   \n\n       overall_sentiment_score  BTC_sentiment  ETH_sentiment  BTC_relevance   \n0                    -0.138797      -0.118125       0.000000       0.018982  \\\n1                     0.021230       0.000000       0.000892       0.000000   \n2                    -0.031243       0.006883       0.000000       0.063967   \n3                    -0.174572      -0.009523       0.000000       0.125701   \n4                    -0.036470       0.001776       0.001233       0.162677   \n...                        ...            ...            ...            ...   \n15178                 0.026801      -0.334158       0.000000       0.235823   \n15179                 0.188588       0.000000       0.029589       0.000000   \n15180                 0.154257       0.221177       0.000000       0.803061   \n15181                -0.009736      -0.256727       0.350953       0.733479   \n15182                -0.024272      -0.029587       0.000000       0.099476   \n\n       ETH_relevance  \n0           0.000000  \n1           0.053967  \n2           0.000000  \n3           0.000000  \n4           0.023400  \n...              ...  \n15178       0.000000  \n15179       0.350564  \n15180       0.000000  \n15181       0.421485  \n15182       0.000000  \n\n[15183 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>XRP-PERP</th>\n      <th>SOL-PERP</th>\n      <th>ETH-PERP</th>\n      <th>DOGE-PERP</th>\n      <th>BTC-PERP</th>\n      <th>timestamp</th>\n      <th>n_comments</th>\n      <th>hour</th>\n      <th>day</th>\n      <th>month</th>\n      <th>overall_sentiment_score</th>\n      <th>BTC_sentiment</th>\n      <th>ETH_sentiment</th>\n      <th>BTC_relevance</th>\n      <th>ETH_relevance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>611127</td>\n      <td>0.030873</td>\n      <td>0.003136</td>\n      <td>-0.003228</td>\n      <td>0.000491</td>\n      <td>0.008013</td>\n      <td>2022-03-01 12:58:00</td>\n      <td>0</td>\n      <td>12</td>\n      <td>1</td>\n      <td>3</td>\n      <td>-0.138797</td>\n      <td>-0.118125</td>\n      <td>0.000000</td>\n      <td>0.018982</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>611819</td>\n      <td>0.000000</td>\n      <td>-0.001285</td>\n      <td>0.006197</td>\n      <td>-0.000774</td>\n      <td>0.028552</td>\n      <td>2022-03-02 00:30:00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0.021230</td>\n      <td>0.000000</td>\n      <td>0.000892</td>\n      <td>0.000000</td>\n      <td>0.053967</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>611941</td>\n      <td>0.004518</td>\n      <td>0.008197</td>\n      <td>0.001328</td>\n      <td>0.000801</td>\n      <td>0.006267</td>\n      <td>2022-03-02 02:32:00</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>-0.031243</td>\n      <td>0.006883</td>\n      <td>0.000000</td>\n      <td>0.063967</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>612269</td>\n      <td>0.012173</td>\n      <td>0.002481</td>\n      <td>0.000525</td>\n      <td>-0.006094</td>\n      <td>0.014352</td>\n      <td>2022-03-02 08:00:00</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2</td>\n      <td>3</td>\n      <td>-0.174572</td>\n      <td>-0.009523</td>\n      <td>0.000000</td>\n      <td>0.125701</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>612389</td>\n      <td>-0.002718</td>\n      <td>-0.002229</td>\n      <td>0.000740</td>\n      <td>0.001686</td>\n      <td>-0.002714</td>\n      <td>2022-03-02 10:00:00</td>\n      <td>0</td>\n      <td>10</td>\n      <td>2</td>\n      <td>3</td>\n      <td>-0.036470</td>\n      <td>0.001776</td>\n      <td>0.001233</td>\n      <td>0.162677</td>\n      <td>0.023400</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15178</th>\n      <td>875067</td>\n      <td>0.000451</td>\n      <td>0.000326</td>\n      <td>0.000977</td>\n      <td>-0.000086</td>\n      <td>0.000418</td>\n      <td>2022-08-31 20:41:00</td>\n      <td>0</td>\n      <td>20</td>\n      <td>2</td>\n      <td>8</td>\n      <td>0.026801</td>\n      <td>-0.334158</td>\n      <td>0.000000</td>\n      <td>0.235823</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>15179</th>\n      <td>875071</td>\n      <td>-0.001382</td>\n      <td>0.000054</td>\n      <td>-0.000488</td>\n      <td>-0.000031</td>\n      <td>0.000070</td>\n      <td>2022-08-31 20:45:00</td>\n      <td>0</td>\n      <td>20</td>\n      <td>2</td>\n      <td>8</td>\n      <td>0.188588</td>\n      <td>0.000000</td>\n      <td>0.029589</td>\n      <td>0.000000</td>\n      <td>0.350564</td>\n    </tr>\n    <tr>\n      <th>15180</th>\n      <td>875094</td>\n      <td>-0.000662</td>\n      <td>-0.000571</td>\n      <td>-0.000490</td>\n      <td>-0.000508</td>\n      <td>-0.000465</td>\n      <td>2022-08-31 21:08:00</td>\n      <td>0</td>\n      <td>21</td>\n      <td>2</td>\n      <td>8</td>\n      <td>0.154257</td>\n      <td>0.221177</td>\n      <td>0.000000</td>\n      <td>0.803061</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>15181</th>\n      <td>875118</td>\n      <td>0.000090</td>\n      <td>0.000981</td>\n      <td>0.000692</td>\n      <td>-0.000022</td>\n      <td>0.000978</td>\n      <td>2022-08-31 21:32:00</td>\n      <td>0</td>\n      <td>21</td>\n      <td>2</td>\n      <td>8</td>\n      <td>-0.009736</td>\n      <td>-0.256727</td>\n      <td>0.350953</td>\n      <td>0.733479</td>\n      <td>0.421485</td>\n    </tr>\n    <tr>\n      <th>15182</th>\n      <td>875221</td>\n      <td>0.000718</td>\n      <td>0.000780</td>\n      <td>-0.000580</td>\n      <td>0.000807</td>\n      <td>0.000185</td>\n      <td>2022-08-31 23:15:00</td>\n      <td>0</td>\n      <td>23</td>\n      <td>2</td>\n      <td>8</td>\n      <td>-0.024272</td>\n      <td>-0.029587</td>\n      <td>0.000000</td>\n      <td>0.099476</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>15183 rows Ã— 16 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortmg = mg.loc[0:]\n",
    "shortmg.dropna()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T17:10:53.199629Z",
     "end_time": "2023-05-06T17:10:53.212954Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "class MyTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data.loc[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "# data = MyTimeSeriesDataset(X_data=shortmg[['n_comments', 'XRP-PERP', 'ETH-PERP']], y_data=shortmg['BTC-PERP'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T17:10:53.214039Z",
     "end_time": "2023-05-06T17:10:53.216126Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['overall_sentiment_score', 'BTC_sentiment', 'ETH_sentiment',\n       'BTC_relevance', 'ETH_relevance', 'timestamp'],\n      dtype='object')"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alp.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T17:10:53.216559Z",
     "end_time": "2023-05-06T17:10:53.234368Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 1, 1, 1, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset into a pandas DataFrame called 'data'\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Scale the features using MinMaxScaler\n",
    "cs = ['BTC-PERP', 'XRP-PERP', 'ETH-PERP', 'SOL-PERP', 'DOGE-PERP', 'overall_sentiment_score', 'BTC_sentiment',\n",
    "      'ETH_sentiment', 'BTC_relevance', 'ETH_relevance']\n",
    "scaled_data = shortmg[cs + [\"hour\", \"day\", \"month\", \"n_comments\"]]\n",
    "# scaled_data = shortmg[['XRP-PERP', 'ETH-PERP', 'BTC-PERP', 'SOL-PERP', 'DOGE-PERP', 'n_comments']].to_numpy()\n",
    "\n",
    "# scaled_data = abs(scaled_data)\n",
    "\n",
    "\n",
    "# scaled_data = scaled_data.to_numpy()\n",
    "#\n",
    "scaler = StandardScaler()\n",
    "#\n",
    "scaled_data = scaled_data.to_numpy()\n",
    "# scaled_data = scaler.fit_transform(scaled_data)\n",
    "\n",
    "\n",
    "# Create sequences and targets for the RNN model\n",
    "seq_length = 60\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "future = 0\n",
    "\n",
    "for i in range(len(scaled_data) - seq_length - future):\n",
    "    X_data.append(scaled_data[i:i + seq_length].flatten())\n",
    "    y_data.append(scaled_data[i + seq_length + future])\n",
    "\n",
    "x_data = scaler.fit_transform(X_data)\n",
    "# y_data = np.delete(y_data, [-1, -2, -3, -4], axis=1)\n",
    "# y_data = np.delete(y_data, [-1, -2, -3, -4, -5, -6, -7, -8], axis=1)\n",
    "y_data = np.array(y_data).T[0].T\n",
    "# to make classifier\n",
    "epsilon = 0.0\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    if x >= epsilon:\n",
    "        return 1\n",
    "    elif x < -epsilon:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "y_data = list(map(f, y_data))\n",
    "\n",
    "print(y_data[1:10])\n",
    "X_data, y_data = np.array(X_data), np.array(y_data)\n",
    "\n",
    "y_data = y_data.astype(np.int64)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T17:16:56.482752Z",
     "end_time": "2023-05-06T17:16:57.508265Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "train_size = int(len(X_data) * 0.8)\n",
    "X_train, X_test = X_data[:train_size], X_data[train_size:]\n",
    "y_train, y_test = y_data[:train_size], y_data[train_size:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T17:16:58.056104Z",
     "end_time": "2023-05-06T17:16:58.058860Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "(Counter({2: 5786, 1: 6312}), Counter({2: 1421, 1: 1604}))"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(y_train), Counter(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T17:16:58.471509Z",
     "end_time": "2023-05-06T17:16:58.476483Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dario/PycharmProjects/BAINSACryptocurrencies/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.5064202318554772, 0.49950413223140494)"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, balanced_accuracy_score, accuracy_score, confusion_matrix\n",
    "\n",
    "# regressor = RandomForestRegressor(random_state=0, max_depth=5, n_estimators=20)\n",
    "# regressor = SVR()\n",
    "regressor = LogisticRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "pred = regressor.predict(X_test)\n",
    "\n",
    "balanced_accuracy_score(pred, y_test), accuracy_score(pred, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T17:17:03.181274Z",
     "end_time": "2023-05-06T17:17:04.828097Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[636, 968],\n       [546, 875]])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T17:17:04.812196Z",
     "end_time": "2023-05-06T17:17:04.828326Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 256\n",
    "train_dataset = TimeSeriesDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
    "test_dataset = TimeSeriesDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T17:17:31.376640Z",
     "end_time": "2023-05-06T17:17:31.421323Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# classifier\n",
    "# y_data = y_data.flatten()\n",
    "# y_data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T17:17:31.901794Z",
     "end_time": "2023-05-06T17:17:31.911971Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, d_model, nhead, num_layers, output_size):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(input_size, d_model)\n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(d_model, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x\n",
    "\n",
    "\n",
    "class DenseModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(input_size, 256)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.flatten(x, 1)\n",
    "        x = self.lin(x)\n",
    "        x = self.ff(x)\n",
    "        o = f.log_softmax(x, dim=1)\n",
    "        return o"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T17:17:32.252740Z",
     "end_time": "2023-05-06T17:17:32.260210Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "(840, 3)"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import optim\n",
    "import torch.nn.functional as f\n",
    "\n",
    "input_size = X_data.shape[1]\n",
    "# hidden_size = 256\n",
    "# num_layers = 8\n",
    "# output_size = y_data.shape[1]\n",
    "output_size = 3\n",
    "# output_size = 1\n",
    "\n",
    "# model = RNNModel(input_size, hidden_size, num_layers, output_size)\n",
    "# model = TransformerModel(input_size, d_model=32, nhead=2, num_layers=2, output_size=output_size)\n",
    "model = DenseModel(input_size, output_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "input_size, output_size"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T17:17:32.718763Z",
     "end_time": "2023-05-06T17:17:32.726113Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T17:17:33.160280Z",
     "end_time": "2023-05-06T17:17:33.171990Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n",
      "Epoch: 2, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n",
      "Epoch: 3, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n",
      "Epoch: 4, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n",
      "Epoch: 5, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n",
      "Epoch: 6, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n",
      "Epoch: 7, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n",
      "Epoch: 8, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n",
      "Epoch: 9, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n",
      "Epoch: 10, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n",
      "Epoch: 11, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n",
      "Epoch: 12, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n",
      "Epoch: 13, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n",
      "Epoch: 14, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n",
      "Epoch: 15, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n",
      "Epoch: 16, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n",
      "Epoch: 17, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n",
      "Epoch: 18, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n",
      "Epoch: 19, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n",
      "Epoch: 20, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n",
      "Epoch: 21, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n",
      "Epoch: 22, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n",
      "Epoch: 23, Train Loss: 0.7142, Test Loss: 0.7147, accuracy: 46.97520661157025\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[53], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, (X_batch, y_batch) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[0;32m---> 12\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m     14\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/PycharmProjects/BAINSACryptocurrencies/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/BAINSACryptocurrencies/venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:82\u001B[0m, in \u001B[0;36mOptimizedModule.forward\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 82\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdynamo_ctx\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_orig_mod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/BAINSACryptocurrencies/venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:209\u001B[0m, in \u001B[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    207\u001B[0m dynamic_ctx\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__enter__\u001B[39m()\n\u001B[1;32m    208\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 209\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    211\u001B[0m     set_eval_frame(prior)\n",
      "Cell \u001B[0;32mIn[51], line 49\u001B[0m, in \u001B[0;36mDenseModel.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlin \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(input_size, \u001B[38;5;241m256\u001B[39m)\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mff \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSequential(\n\u001B[1;32m     37\u001B[0m         nn\u001B[38;5;241m.\u001B[39mReLU(),\n\u001B[1;32m     38\u001B[0m         nn\u001B[38;5;241m.\u001B[39mLinear(\u001B[38;5;241m256\u001B[39m, \u001B[38;5;241m128\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     46\u001B[0m         nn\u001B[38;5;241m.\u001B[39mLinear(\u001B[38;5;241m32\u001B[39m, output_size)\n\u001B[1;32m     47\u001B[0m     )\n\u001B[0;32m---> 49\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;66;03m# x = torch.flatten(x, 1)\u001B[39;00m\n\u001B[1;32m     51\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlin(x)\n\u001B[1;32m     52\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mff(x)\n",
      "File \u001B[0;32m~/PycharmProjects/BAINSACryptocurrencies/venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:209\u001B[0m, in \u001B[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    207\u001B[0m dynamic_ctx\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__enter__\u001B[39m()\n\u001B[1;32m    208\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 209\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    211\u001B[0m     set_eval_frame(prior)\n",
      "File \u001B[0;32m~/PycharmProjects/BAINSACryptocurrencies/venv/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:2819\u001B[0m, in \u001B[0;36maot_module_simplified.<locals>.forward\u001B[0;34m(*runtime_args)\u001B[0m\n\u001B[1;32m   2817\u001B[0m full_args\u001B[38;5;241m.\u001B[39mextend(params_flat)\n\u001B[1;32m   2818\u001B[0m full_args\u001B[38;5;241m.\u001B[39mextend(runtime_args)\n\u001B[0;32m-> 2819\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompiled_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfull_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/BAINSACryptocurrencies/venv/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1222\u001B[0m, in \u001B[0;36mmake_boxed_func.<locals>.g\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m   1221\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mg\u001B[39m(args):\n\u001B[0;32m-> 1222\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/BAINSACryptocurrencies/venv/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:2386\u001B[0m, in \u001B[0;36maot_dispatch_autograd.<locals>.debug_compiled_function\u001B[0;34m(*args)\u001B[0m\n\u001B[1;32m   2380\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m can_require_grad:\n\u001B[1;32m   2381\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m a\u001B[38;5;241m.\u001B[39mrequires_grad, format_guard_bug_msg(\n\u001B[1;32m   2382\u001B[0m             aot_config,\n\u001B[1;32m   2383\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdescribe_input(i,\u001B[38;5;250m \u001B[39maot_config)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m would not require grad\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2384\u001B[0m         )\n\u001B[0;32m-> 2386\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompiled_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/BAINSACryptocurrencies/venv/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1898\u001B[0m, in \u001B[0;36mcreate_runtime_wrapper.<locals>.runtime_wrapper\u001B[0;34m(*args)\u001B[0m\n\u001B[1;32m   1895\u001B[0m     args_with_synthetic_bases \u001B[38;5;241m=\u001B[39m args\n\u001B[1;32m   1897\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39m_force_original_view_tracking(\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m-> 1898\u001B[0m     all_outs \u001B[38;5;241m=\u001B[39m \u001B[43mcall_func_with_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1899\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcompiled_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1900\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs_with_synthetic_bases\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1901\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisable_amp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1902\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1904\u001B[0m num_mutated_inps \u001B[38;5;241m=\u001B[39m runtime_metadata\u001B[38;5;241m.\u001B[39mnum_mutated_inputs\n\u001B[1;32m   1905\u001B[0m num_metadata_mutated_inps \u001B[38;5;241m=\u001B[39m runtime_metadata\u001B[38;5;241m.\u001B[39mnum_mutated_metadata_inputs\n",
      "File \u001B[0;32m~/PycharmProjects/BAINSACryptocurrencies/venv/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1247\u001B[0m, in \u001B[0;36mcall_func_with_args\u001B[0;34m(f, args, steal_args, disable_amp)\u001B[0m\n\u001B[1;32m   1245\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1246\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(f, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_boxed_call\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1247\u001B[0m         out \u001B[38;5;241m=\u001B[39m normalize_as_list(\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1248\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1249\u001B[0m         \u001B[38;5;66;03m# TODO: Please remove soon\u001B[39;00m\n\u001B[1;32m   1250\u001B[0m         \u001B[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001B[39;00m\n\u001B[1;32m   1251\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   1252\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYour compiler for AOTAutograd is returning a a function that doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt take boxed arguments. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1253\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1254\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1255\u001B[0m         )\n",
      "File \u001B[0;32m~/PycharmProjects/BAINSACryptocurrencies/venv/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1222\u001B[0m, in \u001B[0;36mmake_boxed_func.<locals>.g\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m   1221\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mg\u001B[39m(args):\n\u001B[0;32m-> 1222\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/BAINSACryptocurrencies/venv/lib/python3.10/site-packages/torch/autograd/function.py:506\u001B[0m, in \u001B[0;36mFunction.apply\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m    503\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_are_functorch_transforms_active():\n\u001B[1;32m    504\u001B[0m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[1;32m    505\u001B[0m     args \u001B[38;5;241m=\u001B[39m _functorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39munwrap_dead_wrappers(args)\n\u001B[0;32m--> 506\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m    508\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39msetup_context \u001B[38;5;241m==\u001B[39m _SingleLevelFunction\u001B[38;5;241m.\u001B[39msetup_context:\n\u001B[1;32m    509\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    510\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    511\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    512\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstaticmethod. For more details, please see \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/BAINSACryptocurrencies/venv/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:2151\u001B[0m, in \u001B[0;36maot_dispatch_autograd.<locals>.CompiledFunction.forward\u001B[0;34m(ctx, *deduped_flat_tensor_args)\u001B[0m\n\u001B[1;32m   2143\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m   2144\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(ctx, \u001B[38;5;241m*\u001B[39mdeduped_flat_tensor_args):\n\u001B[1;32m   2145\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2149\u001B[0m     \u001B[38;5;66;03m# - Note that in the synthetic bases case, mutated_inputs will correspond to an updated version\u001B[39;00m\n\u001B[1;32m   2150\u001B[0m     \u001B[38;5;66;03m#   of the original view, and not the synthetic base\u001B[39;00m\n\u001B[0;32m-> 2151\u001B[0m     fw_outs \u001B[38;5;241m=\u001B[39m \u001B[43mcall_func_with_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2152\u001B[0m \u001B[43m        \u001B[49m\u001B[43mCompiledFunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompiled_fw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2153\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdeduped_flat_tensor_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2154\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisable_amp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable_amp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2155\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2157\u001B[0m     num_outputs \u001B[38;5;241m=\u001B[39m CompiledFunction\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mnum_outputs\n\u001B[1;32m   2158\u001B[0m     num_outputs_aliased_to_inputs \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2159\u001B[0m         CompiledFunction\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mnum_outputs_aliased_to_inputs\n\u001B[1;32m   2160\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/BAINSACryptocurrencies/venv/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1247\u001B[0m, in \u001B[0;36mcall_func_with_args\u001B[0;34m(f, args, steal_args, disable_amp)\u001B[0m\n\u001B[1;32m   1245\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1246\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(f, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_boxed_call\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1247\u001B[0m         out \u001B[38;5;241m=\u001B[39m normalize_as_list(\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1248\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1249\u001B[0m         \u001B[38;5;66;03m# TODO: Please remove soon\u001B[39;00m\n\u001B[1;32m   1250\u001B[0m         \u001B[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001B[39;00m\n\u001B[1;32m   1251\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   1252\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYour compiler for AOTAutograd is returning a a function that doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt take boxed arguments. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1253\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1254\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1255\u001B[0m         )\n",
      "File \u001B[0;32m/tmp/torchinductor_dario/34/c34vrjvpth4uwujckzr5bxlgk2g77vp4fpaul3vpyh2lqlmfqxbz.py:217\u001B[0m, in \u001B[0;36mcall\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m    215\u001B[0m args\u001B[38;5;241m.\u001B[39mclear()\n\u001B[1;32m    216\u001B[0m buf0 \u001B[38;5;241m=\u001B[39m empty_strided((\u001B[38;5;241m256\u001B[39m, \u001B[38;5;241m256\u001B[39m), (\u001B[38;5;241m256\u001B[39m, \u001B[38;5;241m1\u001B[39m), device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m--> 217\u001B[0m \u001B[43mextern_kernels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maddmm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprimals_2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprimals_13\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mas_strided\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprimals_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m840\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m256\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m840\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbeta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbuf0\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m primals_1\n\u001B[1;32m    219\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m primals_2\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "num_epochs = 30\n",
    "log = False\n",
    "#\n",
    "if \"Linux\" in platform.platform():\n",
    "    model = torch.compile(model)\n",
    "#\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        y_pred = model(X_batch)\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "        y_batch = y_batch.type(torch.LongTensor)\n",
    "        loss = f.nll_loss(y_pred, y_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # optimizer.step()\n",
    "\n",
    "        if batch_idx % 20 == 0 and log:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(X_batch), len(train_loader.dataset), 100. * batch_idx / len(train_loader),\n",
    "                loss))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        test_loss = 0\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            y_batch = y_batch.type(torch.LongTensor)\n",
    "\n",
    "            loss = f.nll_loss(y_pred, y_batch)\n",
    "            test_loss += loss\n",
    "\n",
    "            pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(y_batch.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss = test_loss / len(test_loader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch + 1}, Train Loss: {loss.item():.4f}, Test Loss: {test_loss:.4f}, accuracy: {100 * correct / len(test_loader.dataset)}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T17:08:54.595735Z",
     "end_time": "2023-05-06T17:09:12.112050Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T16:43:54.474881Z",
     "end_time": "2023-05-06T16:43:54.475180Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
